# Profile Exploded Identifiers

This repository contains **`profile_exploded.py`**, a high‑performance script for profiling and identifying "exploded" variants of key personal identifiers in large datasets.

## Overview

Large datasets often contain natural variations and historical values in fields such as names, dates, postcodes, and more. Understanding these variations helps:

* **Characterize** the distribution of real‑world data inconsistencies (metadata.json)
* **Drive deduplication** by focusing on truly distinct variants (explosions.json) - to minimise combinations of historical identifiers

## Context

- to run in > 100M of records, creating potentially x 20 times size of data.


## Files

* **`profile_explode.py`**: Main script with four phases:

  1. **Aggregate** unique `(id, field, variant)` frequencies
  2. **Select** each record's reference values (e.g., latest timestamp)
  3. **Compute** string metrics on the compact variant set (Levenshtein, Jaro–Winkler, date diff)
  4. **Identify** per‑ID "explosions": combinations of `(field, variant)` to include in dedupe

* **`README.md`**: This documentation.

## Requirements

* Python 3.8+
* pandas
* rapidfuzz
* python-dateutil

Install dependencies:

```bash
pip install pandas rapidfuzz python-dateutil pyarrow
```

## Usage

```bash
python profile_explode.py \
  --input exploded_data.parquet \
  --ts-col updated_at \
  --output metadata.json \
  --explosions-output explosions.json
```

* **`--input`**: Path to exploded Parquet dataset. Must include `id`, a timestamp column, and identifier fields.
* **`--ts-col`**: Column to use for selecting each record’s reference (default: `ts`).
* **`--fields`**: List of identifier columns to profile (default: first\_name, last\_name, sex, dob, address, postcode, telephone, email).

## Outputs

### `metadata.json`

A nested JSON structure detailing, for each field and reference value:

```json
{
  "first_name": {
    "jahn": {
      "variants": {
        "john": { "frequency": 10, "edit_distance": 1, "low_similarity": true, "mismatch_first4": true },
        "jon":  { ... }
      }
    },
    "johnny": { ... }
  },
  "dob": {
    "1985-06-15": {
      "variants": {
        "15-06-1985": { "frequency": 5, "exploded": false },
        "1985/06/15": { "frequency": 3, "exploded": false }
      }
    }
  },
  ...
}
```

**Purpose**:

* Understand natural variations and error distributions in your identifiers.
* Guide data quality assessments, schema design, and matching strategies.

### `explosions.json`

A mapping of each record **ID** to the list of `(field, variant)` pairs deemed worthy of deduplication:

```json
{
  "1": [["first_name","jon"], ["postcode","sw1a 1aa"], ["email","john.smith@example.co.uk"]],
  "2": [["last_name","smoth"], ["dob","1985-06-16"]],
  ...
}
```
**Inclusion Rules**:

* If no variants meet the explosion criteria, the reference value is included to ensure every field participates in the deduplication
* all other fields include every variant by default - see next steps 


**Purpose**:

* Direct input to our deduplication pipeline.
* Only explode on fields/values that represent big discrepancies not captured by string comparisons.

## Next Steps

* **Adjust thresholds**: Modify the Jaro–Winkler cutoff or add additional rules in `profile_explode.py`.
* **Integrate** with Dedupe frameworks by feeding in `explosions.json`.
* **Extend** to additional identifier types or custom similarity metrics for different fields.

---

*Generated by profile\_explode.py tooling*
